{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.object = object    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting roi_extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile roi_extract.py\n",
    "\n",
    "# Separated in .py file instead of a notebook cell for easier multiprocessing (e.g spawn)\n",
    "import os\n",
    "os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "sys.path.append('/kaggle/tmp/libs/')\n",
    "from torch2trt import TRTModule\n",
    "from torch.nn import functional as F\n",
    "\n",
    "_TORCH_VER = [int(x) for x in torch.__version__.split(\".\")[:2]]\n",
    "_TORCH11X = (_TORCH_VER >= [1, 10])\n",
    "\n",
    "\n",
    "def meshgrid(*tensors):\n",
    "    if _TORCH11X:\n",
    "        return torch.meshgrid(*tensors, indexing=\"ij\")\n",
    "    else:\n",
    "        return torch.meshgrid(*tensors)\n",
    "\n",
    "\n",
    "def extract_roi_otsu(img, gkernel=(5, 5)):\n",
    "    \"\"\"WARNING: this function modify input image inplace.\"\"\"\n",
    "    ori_h, ori_w = img.shape[:2]\n",
    "    # clip percentile: implant, white lines\n",
    "    upper = np.percentile(img, 95)\n",
    "    img[img > upper] = np.min(img)\n",
    "    # Gaussian filtering to reduce noise (optional)\n",
    "    if gkernel is not None:\n",
    "        img = cv2.GaussianBlur(img, gkernel, 0)\n",
    "    _, img_bin = cv2.threshold(img, 0, 255,\n",
    "                               cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # dilation to improve contours connectivity\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3), (-1, -1))\n",
    "    img_bin = cv2.dilate(img_bin, element)\n",
    "    cnts, _ = cv2.findContours(img_bin, cv2.RETR_EXTERNAL,\n",
    "                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cnts) == 0:\n",
    "        return None, None, None\n",
    "    areas = np.array([cv2.contourArea(cnt) for cnt in cnts])\n",
    "    select_idx = np.argmax(areas)\n",
    "    cnt = cnts[select_idx]\n",
    "    area_pct = areas[select_idx] / (img.shape[0] * img.shape[1])\n",
    "    x0, y0, w, h = cv2.boundingRect(cnt)\n",
    "    # min-max for safety only\n",
    "    # x0, y0, x1, y1\n",
    "    x1 = min(max(int(x0 + w), 0), ori_w)\n",
    "    y1 = min(max(int(y0 + h), 0), ori_h)\n",
    "    x0 = min(max(int(x0), 0), ori_w)\n",
    "    y0 = min(max(int(y0), 0), ori_h)\n",
    "    return [x0, y0, x1, y1], area_pct, None\n",
    "\n",
    "\n",
    "class RoiExtractor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 engine_path,\n",
    "                 input_size,\n",
    "                 num_classes,\n",
    "                 conf_thres=0.5,\n",
    "                 nms_thres=0.9,\n",
    "                 class_agnostic=False,\n",
    "                 area_pct_thres=0.04,\n",
    "                 hw=None,\n",
    "                 strides=None,\n",
    "                 exp=None):\n",
    "        self.input_size = input_size\n",
    "        self.input_h, self.input_w = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_thres = conf_thres\n",
    "        self.nms_thres = nms_thres\n",
    "        self.class_agnostic = class_agnostic\n",
    "        self.area_pct_thres = area_pct_thres\n",
    "\n",
    "        model = TRTModule()\n",
    "        model.load_state_dict(torch.load(engine_path))\n",
    "        self.model = model\n",
    "        if hw is None or strides is None:\n",
    "            assert exp is not None\n",
    "            self._set_meta(exp)\n",
    "        else:\n",
    "            self.hw = hw\n",
    "            self.strides = strides\n",
    "\n",
    "    def _set_meta(self, exp):\n",
    "        assert exp is not None\n",
    "        print(\"Start probing model metadata..\")\n",
    "        # dummy infer\n",
    "        torch_model = exp.get_model().cuda().eval()\n",
    "        _dummy = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "        torch_model(_dummy)\n",
    "        # set attributes\n",
    "        self.hw = torch_model.head.hw\n",
    "        self.strides = torch_model.head.strides\n",
    "        # cleanup\n",
    "        del torch_model, _dummy\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('Done probbing model metadata..')\n",
    "\n",
    "    def decode_outputs(self, outputs):\n",
    "        dtype = outputs.type()\n",
    "        grids = []\n",
    "        strides = []\n",
    "        for (hsize, wsize), stride in zip(self.hw, self.strides):\n",
    "            yv, xv = meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "            grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "            grids.append(grid)\n",
    "            shape = grid.shape[:2]\n",
    "            strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "        grids = torch.cat(grids, dim=1).type(dtype)\n",
    "        strides = torch.cat(strides, dim=1).type(dtype)\n",
    "\n",
    "        outputs = torch.cat(\n",
    "            [(outputs[..., 0:2] + grids) * strides,\n",
    "             torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]],\n",
    "            dim=-1)\n",
    "        return outputs\n",
    "\n",
    "    def post_process(self,\n",
    "                     pred,\n",
    "                     conf_thres=0.5,\n",
    "                     nms_thres=0.9,\n",
    "                     class_agnostic=False):\n",
    "        box_corner = pred.new(pred.shape)\n",
    "        box_corner[:, :, 0] = pred[:, :, 0] - pred[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = pred[:, :, 1] - pred[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = pred[:, :, 0] + pred[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = pred[:, :, 1] + pred[:, :, 3] / 2\n",
    "        pred[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "        output = [None for _ in range(len(pred))]\n",
    "        for i, image_pred in enumerate(pred):\n",
    "\n",
    "            # If none are remaining => process next image\n",
    "            if not image_pred.size(0):\n",
    "                continue\n",
    "            # Get score and class with highest confidence\n",
    "            class_conf, class_pred = torch.max(image_pred[:, 5:5 +\n",
    "                                                          self.num_classes],\n",
    "                                               1,\n",
    "                                               keepdim=True)\n",
    "\n",
    "            conf_mask = (image_pred[:, 4] * class_conf.squeeze() >=\n",
    "                         conf_thres).squeeze()\n",
    "            # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "            detections = torch.cat(\n",
    "                (image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "            detections = detections[conf_mask]\n",
    "            if not detections.size(0):\n",
    "                continue\n",
    "\n",
    "            if class_agnostic:\n",
    "                nms_out_index = torchvision.ops.nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    nms_thres,\n",
    "                )\n",
    "            else:\n",
    "                nms_out_index = torchvision.ops.batched_nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    detections[:, 6],\n",
    "                    nms_thres,\n",
    "                )\n",
    "            detections = detections[nms_out_index]\n",
    "            if output[i] is None:\n",
    "                output[i] = detections\n",
    "            else:\n",
    "                output[i] = torch.cat((output[i], detections))\n",
    "        return output\n",
    "\n",
    "    def preprocess_single(self, img: torch.Tensor):\n",
    "        ori_h = img.size(0)\n",
    "        ori_w = img.size(1)\n",
    "        ratio = min(self.input_h / ori_h, self.input_w / ori_w)\n",
    "        # resize\n",
    "        resized_img = F.interpolate(img.view(1, 1, ori_h, ori_w),\n",
    "                                    mode=\"bilinear\",\n",
    "                                    scale_factor=ratio,\n",
    "                                    recompute_scale_factor=True)[0, 0]\n",
    "        # padding\n",
    "        padded_img = torch.full((self.input_h, self.input_w),\n",
    "                                114,\n",
    "                                dtype=resized_img.dtype,\n",
    "                                device='cuda')\n",
    "        padded_img[:resized_img.size(0), :resized_img.size(1)] = resized_img\n",
    "        # 1 channel --> 3 channels\n",
    "        padded_img = padded_img.unsqueeze(-1).expand(-1, -1, 3)\n",
    "        # HWC --> CHW\n",
    "        padded_img = padded_img.permute(2, 0, 1)\n",
    "        padded_img = padded_img.float()\n",
    "        return padded_img, resized_img, ratio, ori_h, ori_w\n",
    "\n",
    "    def detect_single(self, img):\n",
    "        padded_img, resized_img, ratio, ori_h, ori_w = self.preprocess_single(\n",
    "            img)\n",
    "        padded_img = padded_img.unsqueeze(0)\n",
    "        output = self.model(padded_img)\n",
    "        output = self.decode_outputs(output)\n",
    "        # x0, y0, x1, y1, box_conf, cls_conf, cls_id\n",
    "        output = self.post_process(output, self.conf_thres, self.nms_thres)[0]\n",
    "        if output is not None:\n",
    "            output[:, :4] = output[:, :4] / ratio\n",
    "            # re-compute: conf = box_conf * cls_conf\n",
    "            output[:, 4] = output[:, 4] * output[:, 5]\n",
    "            # select box with highest confident\n",
    "            output = output[output[:, 4].argmax()]\n",
    "            x0 = min(max(int(output[0]), 0), ori_w)\n",
    "            y0 = min(max(int(output[1]), 0), ori_h)\n",
    "            x1 = min(max(int(output[2]), 0), ori_w)\n",
    "            y1 = min(max(int(output[3]), 0), ori_h)\n",
    "            area_pct = (x1 - x0) * (y1 - y0) / (ori_h * ori_w)\n",
    "            if area_pct >= self.area_pct_thres:\n",
    "                # xyxy, area_pct, conf\n",
    "                return [x0, y0, x1, y1], area_pct, output[4]\n",
    "\n",
    "        # if YOLOX fail, try Otsu thresholding + find contours\n",
    "        xyxy, area_pct, _ = extract_roi_otsu(\n",
    "            resized_img.to(torch.uint8).cpu().numpy())\n",
    "        # if both fail, use full frame\n",
    "        if xyxy is not None:\n",
    "            if area_pct >= self.area_pct_thres:\n",
    "                print('ROI detection: using Otsu.')\n",
    "                x0, y0, x1, y1 = xyxy\n",
    "                x0 = min(max(int(x0 / ratio), 0), ori_w)\n",
    "                y0 = min(max(int(y0 / ratio), 0), ori_h)\n",
    "                x1 = min(max(int(x1 / ratio), 0), ori_w)\n",
    "                y1 = min(max(int(y1 / ratio), 0), ori_h)\n",
    "                return [x0, y0, x1, y1], area_pct, None\n",
    "        print('ROI detection: both fail.')\n",
    "        return None, area_pct, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from timm.data import resolve_data_config\n",
    "from timm.models import create_model\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class KFoldEnsembleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_info, ckpt_paths):\n",
    "        super(KFoldEnsembleModel, self).__init__()\n",
    "        fmodels = []\n",
    "        for i, ckpt_path in enumerate(ckpt_paths):\n",
    "            print(f'Loading model from {ckpt_path}')\n",
    "            fmodel = create_model(\n",
    "                model_info['model_name'],\n",
    "                num_classes=model_info['num_classes'],\n",
    "                in_chans=model_info['in_chans'],\n",
    "                pretrained=False,\n",
    "                checkpoint_path=ckpt_path,\n",
    "                global_pool=model_info['global_pool'],\n",
    "            ).eval()\n",
    "            data_config = resolve_data_config({}, model=fmodel)\n",
    "            print('Data config:', data_config)\n",
    "            mean = np.array(data_config['mean']) * 255\n",
    "            std = np.array(data_config['std']) * 255\n",
    "            print(f'mean={mean}, std={std}')\n",
    "            fmodels.append(fmodel)\n",
    "        self.fmodels = nn.ModuleList(fmodels)\n",
    "\n",
    "        self.register_buffer('mean',\n",
    "                             torch.FloatTensor(mean).reshape(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.FloatTensor(std).reshape(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         x = x.sub(self.mean).div(self.std)\n",
    "        x = (x - self.mean) / self.std\n",
    "        probs = []\n",
    "        for fmodel in self.fmodels:\n",
    "            logits = fmodel(x)\n",
    "            #             prob = logits.softmax(dim=1)[:, 1]\n",
    "            prob = logits.sigmoid()[:, 0]\n",
    "            probs.append(prob)\n",
    "        probs = torch.stack(probs, dim=1)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roi_extract\n",
    "\n",
    "# global vars\n",
    "J2K_SUID = '1.2.840.10008.1.2.4.90'\n",
    "J2K_HEADER = b\"\\x00\\x00\\x00\\x0C\"\n",
    "JLL_SUID = '1.2.840.10008.1.2.4.70'\n",
    "JLL_HEADER = b\"\\xff\\xd8\\xff\\xe0\"\n",
    "SUID2HEADER = {J2K_SUID: J2K_HEADER, JLL_SUID: JLL_HEADER}\n",
    "VOILUT_FUNCS_MAP = {'LINEAR': 0, 'LINEAR_EXACT': 1, 'SIGMOID': 2}\n",
    "VOILUT_FUNCS_INV_MAP = {v: k for k, v in VOILUT_FUNCS_MAP.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "# binarization threshold for classification\n",
    "THRES = 0.31\n",
    "AUTO_THRES = False\n",
    "AUTO_THRES_PERCENTILE = 0.97935\n",
    "\n",
    "# classification model\n",
    "USE_TRT = True\n",
    "\n",
    "\n",
    "# roi detection\n",
    "ROI_YOLOX_INPUT_SIZE = [416, 416]\n",
    "ROI_YOLOX_CONF_THRES = 0.5\n",
    "ROI_YOLOX_NMS_THRES = 0.9\n",
    "ROI_YOLOX_HW = [(52, 52), (26, 26), (13, 13)]\n",
    "ROI_YOLOX_STRIDES = [8, 16, 32]\n",
    "ROI_AREA_PCT_THRES = 0.04\n",
    "\n",
    "# model\n",
    "MODEL_INPUT_SIZE = [2048, 1024]\n",
    "\n",
    "MODE = 'KAGGLE-TEST'\n",
    "assert MODE in ['LOCAL-VAL', 'KAGGLE-VAL', 'KAGGLE-TEST']\n",
    "\n",
    "# settings corresponding to each mode\n",
    "if MODE == 'KAGGLE-VAL':\n",
    "    TRT_MODEL_PATH = '/kaggle/input/rsna-breast-cancer-detection-best-ckpts/best_convnext_ensemble_batch2_fp32_torch2trt.engine'\n",
    "    TORCH_MODEL_CKPT_PATHS = [\n",
    "        f'/kaggle/input/rsna-breast-cancer-detection-best-ckpts/best_convnext_fold_{i}.pth.tar'\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    ROI_YOLOX_ENGINE_PATH = '/kaggle/input/rsna-breast-cancer-detection-best-ckpts/yolox_nano_416_roi_trt_p100.pth'\n",
    "    CSV_PATH = '/kaggle/input/rsna-breast-cancer-detection-best-ckpts/_val_fold_0.csv'\n",
    "    DCM_ROOT_DIR = '/kaggle/input/rsna-breast-cancer-detection/train_images'\n",
    "    SAVE_IMG_ROOT_DIR = '/kaggle/tmp/pngs'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = False\n",
    "elif MODE == 'KAGGLE-TEST':\n",
    "    TRT_MODEL_PATH = '/kaggle/input/rsna-breast-cancer-detection-best-ckpts/best_convnext_ensemble_batch2_fp32_torch2trt.engine'\n",
    "    TORCH_MODEL_CKPT_PATHS = [\n",
    "        f'/kaggle/input/rsna-breast-cancer-detection-best-ckpts/best_convnext_fold_{i}.pth.tar'\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    ROI_YOLOX_ENGINE_PATH = '/kaggle/input/rsna-breast-cancer-detection-best-ckpts/yolox_nano_416_roi_trt_p100.pth'\n",
    "    CSV_PATH = '/kaggle/input/rsna-breast-cancer-detection/test.csv'\n",
    "    DCM_ROOT_DIR = '/kaggle/input/rsna-breast-cancer-detection/test_images'\n",
    "    SAVE_IMG_ROOT_DIR = '/kaggle/tmp/pngs'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = True\n",
    "elif MODE == 'LOCAL-VAL':\n",
    "    TRT_MODEL_PATH = './assets/best_convnext_ensemble_batch2_fp32_torch2trt.engine'\n",
    "    TORCH_MODEL_CKPT_PATHS = [\n",
    "        f'./assets/best_convnext_fold_{i}.pth.tar'\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    ROI_YOLOX_ENGINE_PATH = '../roi_det/YOLOX/YOLOX_outputs/yolox_nano_bre_416/model_trt.pth'\n",
    "    CSV_PATH = '../../datasets/cv/v1/val_fold_0.csv'\n",
    "    DCM_ROOT_DIR = '../../datasets/train_images/'\n",
    "    SAVE_IMG_ROOT_DIR = './temp_save'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PydicomMetadata:\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        if \"WindowWidth\" not in ds or \"WindowCenter\" not in ds:\n",
    "            self.window_widths = []\n",
    "            self.window_centers = []\n",
    "        else:\n",
    "            ww = ds['WindowWidth']\n",
    "            wc = ds['WindowCenter']\n",
    "            self.window_widths = [float(e) for e in ww\n",
    "                                  ] if ww.VM > 1 else [float(ww.value)]\n",
    "\n",
    "            self.window_centers = [float(e) for e in wc\n",
    "                                   ] if wc.VM > 1 else [float(wc.value)]\n",
    "\n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = str(ds.get('VOILUTFunction', 'LINEAR')).upper()\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        assert len(self.window_widths) == len(self.window_centers)\n",
    "\n",
    "\n",
    "class DicomsdlMetadata:\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        self.window_widths = ds.WindowWidth\n",
    "        self.window_centers = ds.WindowCenter\n",
    "        if self.window_widths is None or self.window_centers is None:\n",
    "            self.window_widths = []\n",
    "            self.window_centers = []\n",
    "        else:\n",
    "            try:\n",
    "                if not isinstance(self.window_widths, list):\n",
    "                    self.window_widths = [self.window_widths]\n",
    "                self.window_widths = [float(e) for e in self.window_widths]\n",
    "                if not isinstance(self.window_centers, list):\n",
    "                    self.window_centers = [self.window_centers]\n",
    "                self.window_centers = [float(e) for e in self.window_centers]\n",
    "            except:\n",
    "                self.window_widths = []\n",
    "                self.window_centers = []\n",
    "\n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = ds.VOILUTFunction\n",
    "        if self.voilut_func is None:\n",
    "            self.voilut_func = 'LINEAR'\n",
    "        else:\n",
    "            self.voilut_func = str(self.voilut_func).upper()\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        assert len(self.window_widths) == len(self.window_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "# from pydicom's source\n",
    "def _apply_windowing_np_v1(arr,\n",
    "                           window_width=None,\n",
    "                           window_center=None,\n",
    "                           voi_func='LINEAR',\n",
    "                           y_min=0,\n",
    "                           y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "        below = arr <= (window_center - window_width / 2)\n",
    "        above = arr > (window_center + window_width / 2)\n",
    "        between = np.logical_and(~below, ~above)\n",
    "\n",
    "        arr[below] = y_min\n",
    "        arr[above] = y_max\n",
    "        if between.any():\n",
    "            arr[between] = ((\n",
    "                (arr[between] - window_center) / window_width + 0.5) * y_range\n",
    "                            + y_min)\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        arr = y_range / (1 +\n",
    "                         np.exp(-4 *\n",
    "                                (arr - window_center) / window_width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _apply_windowing_np_v2(arr,\n",
    "                           window_width=None,\n",
    "                           window_center=None,\n",
    "                           voi_func='LINEAR',\n",
    "                           y_min=0,\n",
    "                           y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "        # simple trick to improve speed\n",
    "        s = y_range / window_width\n",
    "        b = (-window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * s + b\n",
    "        arr = np.clip(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        # simple trick to improve speed\n",
    "        s = -4 / window_width\n",
    "        arr = y_range / (1 + np.exp((arr - window_center) * s)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _apply_windowing_torch(arr,\n",
    "                           window_width=None,\n",
    "                           window_center=None,\n",
    "                           voi_func='LINEAR',\n",
    "                           y_min=0,\n",
    "                           y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.double()\n",
    "    arr = arr.float()\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "        # simple trick to improve speed\n",
    "        s = y_range / window_width\n",
    "        b = (-window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * s + b\n",
    "        arr = torch.clamp(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        # simple trick to improve speed\n",
    "        s = -4 / window_width\n",
    "        arr = y_range / (1 + torch.exp((arr - window_center) * s)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_windowing(arr,\n",
    "                    window_width=None,\n",
    "                    window_center=None,\n",
    "                    voi_func='LINEAR',\n",
    "                    y_min=0,\n",
    "                    y_max=255,\n",
    "                    backend='np_v2'):\n",
    "    if backend == 'torch':\n",
    "        if isinstance(arr, torch.Tensor):\n",
    "            pass\n",
    "        elif isinstance(arr, np.ndarray):\n",
    "            if arr.dtype == np.uint16:\n",
    "                arr = torch.from_numpy(arr, torch.int16)\n",
    "            else:\n",
    "                arr = torch.from_numpy(arr)\n",
    "\n",
    "    if backend == 'np_v1':\n",
    "        windowing_func = _apply_windowing_np_v1\n",
    "    elif backend == 'np_v2':\n",
    "        windowing_func = _apply_windowing_np_v2\n",
    "    elif backend == 'torch':\n",
    "        windowing_func = _apply_windowing_torch\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'Invalid backend {backend}, must be one of [\"np\", \"np_v2\", \"torch\"]'\n",
    "        )\n",
    "\n",
    "    arr = windowing_func(arr,\n",
    "                         window_width=window_width,\n",
    "                         window_center=window_center,\n",
    "                         voi_func=voi_func,\n",
    "                         y_min=y_min,\n",
    "                         y_max=y_max)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(img):\n",
    "    maxv = img.max()\n",
    "    minv = img.min()\n",
    "    if maxv > minv:\n",
    "        return (img - minv) / (maxv - minv)\n",
    "    else:\n",
    "        return img - minv  # ==0\n",
    "\n",
    "\n",
    "#@TODO: percentile on both min-max?\n",
    "# this version is not correctly implemented, but used in the winning submission\n",
    "def percentile_min_max_scale(img, pct=99):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        maxv = np.percentile(img, pct) - 1\n",
    "        minv = img.min()\n",
    "        assert maxv >= minv\n",
    "        if maxv > minv:\n",
    "            ret = (img - minv) / (maxv - minv)\n",
    "        else:\n",
    "            ret = img - minv  # ==0\n",
    "        ret = np.clip(ret, 0, 1)\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        maxv = torch.quantile(img, pct / 100) - 1\n",
    "        minv = img.min()\n",
    "        assert maxv >= minv\n",
    "        if maxv > minv:\n",
    "            ret = (img - minv) / (maxv - minv)\n",
    "        else:\n",
    "            ret = img - minv  # ==0\n",
    "        ret = torch.clamp(ret, 0, 1)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Invalid img type, should be numpy array or torch.Tensor')\n",
    "    return ret\n",
    "\n",
    "\n",
    "def resize_and_pad(img, input_size=MODEL_INPUT_SIZE):\n",
    "    input_h, input_w = input_size\n",
    "    ori_h, ori_w = img.shape[:2]\n",
    "    ratio = min(input_h / ori_h, input_w / ori_w)\n",
    "    # resize\n",
    "    img = F.interpolate(img.view(1, 1, ori_h, ori_w),\n",
    "                        mode=\"bilinear\",\n",
    "                        scale_factor=ratio,\n",
    "                        recompute_scale_factor=True)[0, 0]\n",
    "    # padding\n",
    "    padded_img = torch.zeros((input_h, input_w),\n",
    "                             dtype=img.dtype,\n",
    "                             device='cuda')\n",
    "    cur_h, cur_w = img.shape\n",
    "    y_start = (input_h - cur_h) // 2\n",
    "    x_start = (input_w - cur_w) // 2\n",
    "    padded_img[y_start:y_start + cur_h, x_start:x_start + cur_w] = img\n",
    "    padded_img = padded_img.unsqueeze(-1).expand(-1, -1, 3)\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "def save_img_to_file(save_path, img, backend='cv2'):\n",
    "    file_ext = os.path.basename(save_path).split('.')[-1]\n",
    "    if backend == 'cv2':\n",
    "        if img.dtype == np.uint16:\n",
    "            # https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce\n",
    "            assert file_ext in ['png', 'jp2', 'tiff', 'tif']\n",
    "            cv2.imwrite(save_path, img)\n",
    "        elif img.dtype == np.uint8:\n",
    "            cv2.imwrite(save_path, img)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                '`cv2` backend only support uint8 or uint16 images.')\n",
    "    elif backend == 'np':\n",
    "        assert file_ext == 'npy'\n",
    "        np.save(save_path, img)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported backend `{backend}`.')\n",
    "\n",
    "\n",
    "def load_img_from_file(img_path, backend='cv2'):\n",
    "    if backend == 'cv2':\n",
    "        return cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n",
    "    elif backend == 'np':\n",
    "        return np.load(img_path)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "        \n",
    "\n",
    "def make_uid_transfer_dict(df, dcm_root_dir):\n",
    "    machine_id_to_transfer = {}\n",
    "    machine_id = df.machine_id.unique()\n",
    "    for i in machine_id:\n",
    "        row = df[df.machine_id == i].iloc[0]\n",
    "        sample_dcm_path = os.path.join(dcm_root_dir, str(row.patient_id),\n",
    "                                       f'{row.image_id}.dcm')\n",
    "        dicom = pydicom.dcmread(sample_dcm_path)\n",
    "        machine_id_to_transfer[i] = dicom.file_meta.TransferSyntaxUID\n",
    "    return machine_id_to_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dali'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DALI patch for INT16 support\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdali\u001b[39;00m\n\u001b[1;32m      5\u001b[0m DALI2TORCH_TYPES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     types\u001b[38;5;241m.\u001b[39mDALIDataType\u001b[38;5;241m.\u001b[39mFLOAT: torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m      7\u001b[0m     types\u001b[38;5;241m.\u001b[39mDALIDataType\u001b[38;5;241m.\u001b[39mFLOAT64: torch\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     types\u001b[38;5;241m.\u001b[39mDALIDataType\u001b[38;5;241m.\u001b[39mINT64: torch\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m TORCH_DTYPES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39muint8,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m     22\u001b[0m }\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dali'"
     ]
    }
   ],
   "source": [
    "# DALI patch for INT16 support\n",
    "################################################################################\n",
    "import types\n",
    "import dali\n",
    "DALI2TORCH_TYPES = {\n",
    "    types.DALIDataType.FLOAT: torch.float32,\n",
    "    types.DALIDataType.FLOAT64: torch.float64,\n",
    "    types.DALIDataType.FLOAT16: torch.float16,\n",
    "    types.DALIDataType.UINT8: torch.uint8,\n",
    "    types.DALIDataType.INT8: torch.int8,\n",
    "    types.DALIDataType.UINT16: torch.int16,\n",
    "    types.DALIDataType.INT16: torch.int16,\n",
    "    types.DALIDataType.INT32: torch.int32,\n",
    "    types.DALIDataType.INT64: torch.int64\n",
    "}\n",
    "\n",
    "TORCH_DTYPES = {\n",
    "    'uint8': torch.uint8,\n",
    "    'float16': torch.float16,\n",
    "    'float32': torch.float32,\n",
    "    'float64': torch.float64,\n",
    "}\n",
    "\n",
    "\n",
    "# @TODO: dangerous to copy from UINT16 to INT16 (memory layout?)\n",
    "# little/big endian ?\n",
    "# @TODO: faster reuse memory without copying: https://github.com/NVIDIA/DALI/issues/4126\n",
    "def feed_ndarray(dali_tensor, arr, cuda_stream=None):\n",
    "    \"\"\"\n",
    "    Copy contents of DALI tensor to PyTorch's Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `dali_tensor` : nvidia.dali.backend.TensorCPU or nvidia.dali.backend.TensorGPU\n",
    "                    Tensor from which to copy\n",
    "    `arr` : torch.Tensor\n",
    "            Destination of the copy\n",
    "    `cuda_stream` : torch.cuda.Stream, cudaStream_t or any value that can be cast to cudaStream_t.\n",
    "                    CUDA stream to be used for the copy\n",
    "                    (if not provided, an internal user stream will be selected)\n",
    "                    In most cases, using pytorch's current stream is expected (for example,\n",
    "                    if we are copying to a tensor allocated with torch.zeros(...))\n",
    "    \"\"\"\n",
    "    dali_type = DALI2TORCH_TYPES[dali_tensor.dtype]\n",
    "\n",
    "    assert dali_type == arr.dtype, (\n",
    "        \"The element type of DALI Tensor/TensorList\"\n",
    "        \" doesn't match the element type of the target PyTorch Tensor: \"\n",
    "        \"{} vs {}\".format(dali_type, arr.dtype))\n",
    "    assert dali_tensor.shape() == list(arr.size()), \\\n",
    "        (\"Shapes do not match: DALI tensor has size {0}, but PyTorch Tensor has size {1}\".\n",
    "            format(dali_tensor.shape(), list(arr.size())))\n",
    "    cuda_stream = types._raw_cuda_stream(cuda_stream)\n",
    "\n",
    "    # turn raw int to a c void pointer\n",
    "    c_type_pointer = ctypes.c_void_p(arr.data_ptr())\n",
    "    if isinstance(dali_tensor, (TensorGPU, TensorListGPU)):\n",
    "        stream = None if cuda_stream is None else ctypes.c_void_p(cuda_stream)\n",
    "        dali_tensor.copy_to_external(c_type_pointer, stream, non_blocking=True)\n",
    "    else:\n",
    "        dali_tensor.copy_to_external(c_type_pointer)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class _JStreamExternalSource:\n",
    "    \"\"\"DALI External Source for in-memory dicom decoding\"\"\"\n",
    "\n",
    "    def __init__(self, dcm_paths, batch_size=1):\n",
    "        self.dcm_paths = dcm_paths\n",
    "        self.len = len(dcm_paths)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, batch_info):\n",
    "        idx = batch_info.iteration\n",
    "        # print('IDX:', batch_info.iteration, batch_info.epoch_idx)\n",
    "        start = idx * self.batch_size\n",
    "        end = min(self.len, start + self.batch_size)\n",
    "        if end <= start:\n",
    "            raise StopIteration()\n",
    "\n",
    "        batch_dcm_paths = self.dcm_paths[start:end]\n",
    "        j_streams = []\n",
    "        inverts = []\n",
    "        windowing_params = []\n",
    "        voilut_funcs = []\n",
    "\n",
    "        for dcm_path in batch_dcm_paths:\n",
    "            ds = pydicom.dcmread(dcm_path)\n",
    "            pixel_data = ds.PixelData\n",
    "            offset = pixel_data.find(\n",
    "                SUID2HEADER[ds.file_meta.TransferSyntaxUID])\n",
    "            j_stream = np.array(bytearray(pixel_data[offset:]), np.uint8)\n",
    "            invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "            meta = PydicomMetadata(ds)\n",
    "            windowing_param = np.array(\n",
    "                [meta.window_centers, meta.window_widths], np.float16)\n",
    "            voilut_func = VOILUT_FUNCS_MAP[meta.voilut_func]\n",
    "            j_streams.append(j_stream)\n",
    "            inverts.append(invert)\n",
    "            windowing_params.append(windowing_param)\n",
    "            voilut_funcs.append(voilut_func)\n",
    "        return j_streams, np.array(inverts, dtype=np.bool_), \\\n",
    "            windowing_params, np.array(voilut_funcs, dtype=np.uint8)\n",
    "\n",
    "\n",
    "@dali.pipeline_def\n",
    "def _dali_pipeline(eii):\n",
    "    jpeg, invert, windowing_param, voilut_func = dali.fn.external_source(\n",
    "        source=eii,\n",
    "        num_outputs=4,\n",
    "        dtype=[\n",
    "            dali.types.UINT8, dali.types.BOOL, dali.types.FLOAT16,\n",
    "            dali.types.UINT8\n",
    "        ],\n",
    "        batch=True,\n",
    "        batch_info=True,\n",
    "        parallel=True)\n",
    "    ori_img = dali.fn.experimental.decoders.image(\n",
    "        jpeg,\n",
    "        device='mixed',\n",
    "        output_type=dali.types.ANY_DATA,\n",
    "        dtype=dali.types.UINT16)\n",
    "    return ori_img, invert, windowing_param, voilut_func\n",
    "\n",
    "\n",
    "def decode_crop_save_dali(roi_yolox_engine_path,\n",
    "                          dcm_paths,\n",
    "                          save_paths,\n",
    "                          save_backend='cv2',\n",
    "                          batch_size=1,\n",
    "                          num_threads=1,\n",
    "                          py_num_workers=1,\n",
    "                          py_start_method='fork',\n",
    "                          device_id=0):\n",
    "    \"\"\"DALI dicom decoding --> ROI cropping --> norm --> save as 8-bits PNG\"\"\"\n",
    "    \n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    assert save_backend in ['cv2', 'np']\n",
    "    num_dcms = len(dcm_paths)\n",
    "\n",
    "    # dali to process with chunk in-memory\n",
    "    external_source = _JStreamExternalSource(dcm_paths, batch_size=batch_size)\n",
    "    pipe = _dali_pipeline(\n",
    "        external_source,\n",
    "        py_num_workers=py_num_workers,\n",
    "        py_start_method=py_start_method,\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_threads,\n",
    "        device_id=device_id,\n",
    "        debug=False,\n",
    "    )\n",
    "    pipe.build()\n",
    "\n",
    "    roi_extractor = roi_extract.RoiExtractor(engine_path=roi_yolox_engine_path,\n",
    "                                             input_size=ROI_YOLOX_INPUT_SIZE,\n",
    "                                             num_classes=1,\n",
    "                                             conf_thres=ROI_YOLOX_CONF_THRES,\n",
    "                                             nms_thres=ROI_YOLOX_NMS_THRES,\n",
    "                                             class_agnostic=False,\n",
    "                                             area_pct_thres=ROI_AREA_PCT_THRES,\n",
    "                                             hw=ROI_YOLOX_HW,\n",
    "                                             strides=ROI_YOLOX_STRIDES,\n",
    "                                             exp=None)\n",
    "    print('ROI extractor (YOLOX) loaded!')\n",
    "\n",
    "    num_batchs = num_dcms // batch_size\n",
    "    last_batch_size = batch_size\n",
    "    if num_dcms % batch_size > 0:\n",
    "        num_batchs += 1\n",
    "        last_batch_size = num_dcms % batch_size\n",
    "\n",
    "    cur_idx = -1\n",
    "    for _batch_idx in tqdm(range(num_batchs)):\n",
    "        try:\n",
    "            outs = pipe.run()\n",
    "        except Exception as e:\n",
    "            #             print('DALI exception occur:', e)\n",
    "            print(\n",
    "                f'Exception: One of {dcm_paths[_batch_idx * batch_size: (_batch_idx + 1) * batch_size]} can not be decoded.'\n",
    "            )\n",
    "            # ignore this batch and re-build pipeline\n",
    "            if _batch_idx < num_batchs - 1:\n",
    "                cur_idx += batch_size\n",
    "                del external_source, pipe\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                external_source = _JStreamExternalSource(\n",
    "                    dcm_paths[(_batch_idx + 1) * batch_size:],\n",
    "                    batch_size=batch_size)\n",
    "                pipe = _dali_pipeline(\n",
    "                    external_source,\n",
    "                    py_num_workers=py_num_workers,\n",
    "                    py_start_method=py_start_method,\n",
    "                    batch_size=batch_size,\n",
    "                    num_threads=num_threads,\n",
    "                    device_id=device_id,\n",
    "                    debug=False,\n",
    "                )\n",
    "                pipe.build()\n",
    "            else:\n",
    "                cur_idx += last_batch_size\n",
    "            continue\n",
    "\n",
    "        imgs = outs[0]\n",
    "        inverts = outs[1]\n",
    "        windowing_params = outs[2]\n",
    "        voilut_funcs = outs[3]\n",
    "        for j in range(len(inverts)):\n",
    "            cur_idx += 1\n",
    "            save_path = save_paths[cur_idx]\n",
    "            img_dali = imgs[j]\n",
    "            img_torch = torch.empty(img_dali.shape(),\n",
    "                                    dtype=torch.int16,\n",
    "                                    device='cuda')\n",
    "            feed_ndarray(img_dali,\n",
    "                         img_torch,\n",
    "                         cuda_stream=torch.cuda.current_stream(device=0))\n",
    "            # @TODO: test whether copy uint16 to int16 pointer is safe in this case\n",
    "            if 0:\n",
    "                img_np = img_dali.as_cpu().squeeze(-1)  # uint16\n",
    "                print(type(img_np), img_np.shape)\n",
    "                img_np = torch.from_numpy(img_np, dtype=torch.int16)\n",
    "                diff = torch.max(torch.abs(img_np - img_torch))\n",
    "                assert diff == 0, f'{img_torch.shape}, {img_np.shape}, {diff}'\n",
    "\n",
    "            invert = inverts.at(j).item()\n",
    "            windowing_param = windowing_params.at(j)\n",
    "            voilut_func = voilut_funcs.at(j).item()\n",
    "            voilut_func = VOILUT_FUNCS_INV_MAP[voilut_func]\n",
    "\n",
    "            # YOLOX for ROI extraction\n",
    "            img_yolox = min_max_scale(img_torch)\n",
    "            img_yolox = (img_yolox * 255)  # float32\n",
    "            if invert:\n",
    "                img_yolox = 255 - img_yolox\n",
    "            # YOLOX infer\n",
    "            # who know if exception happen in hidden test ?\n",
    "            try:\n",
    "                xyxy, _area_pct, _conf = roi_extractor.detect_single(img_yolox)\n",
    "                if xyxy is not None:\n",
    "                    x0, y0, x1, y1 = xyxy\n",
    "                    crop = img_torch[y0:y1, x0:x1]\n",
    "                else:\n",
    "                    crop = img_torch\n",
    "            except:\n",
    "                print('ROI extract exception!')\n",
    "                crop = img_torch\n",
    "\n",
    "            # apply windowing\n",
    "            if windowing_param.shape[1] != 0:\n",
    "                default_window_center = windowing_param[0, 0]\n",
    "                default_window_width = windowing_param[1, 0]\n",
    "                crop = apply_windowing(crop,\n",
    "                                       window_width=default_window_width,\n",
    "                                       window_center=default_window_center,\n",
    "                                       voi_func=voilut_func,\n",
    "                                       y_min=0,\n",
    "                                       y_max=255,\n",
    "                                       backend='torch')\n",
    "            # if no window center/width in dcm file\n",
    "            # do simple min-max scaling\n",
    "            else:\n",
    "                print('No windowing param!')\n",
    "                crop = min_max_scale(crop)\n",
    "                crop = crop * 255\n",
    "            if invert:\n",
    "                crop = 255 - crop\n",
    "            crop = resize_and_pad(crop, MODEL_INPUT_SIZE)\n",
    "            crop = crop.to(torch.uint8)\n",
    "            crop = crop.cpu().numpy()\n",
    "            save_img_to_file(save_path, crop, backend=save_backend)\n",
    "\n",
    "\n",
    "#     assert cur_idx == len(\n",
    "#         save_paths) - 1, f'{cur_idx} != {len(save_paths) - 1}'\n",
    "    try:\n",
    "        del external_source, pipe, roi_extractor\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return\n",
    "\n",
    "\n",
    "def decode_and_save_dali_parallel(\n",
    "        roi_yolox_engine_path,\n",
    "        dcm_paths,\n",
    "        save_paths,\n",
    "        save_backend='cv2',\n",
    "        batch_size=1,\n",
    "        num_threads=1,\n",
    "        py_num_workers=1,\n",
    "        py_start_method='fork',\n",
    "        device_id=0,\n",
    "        parallel_n_jobs=2,\n",
    "        parallel_n_chunks=4,\n",
    "        parallel_backend='joblib',  # joblib or multiprocessing\n",
    "        joblib_backend='loky'):\n",
    "    assert parallel_backend in ['joblib', 'multiprocessing']\n",
    "    assert joblib_backend in ['threading', 'multiprocessing', 'loky']\n",
    "    # py_num_workers > 0 means using multiprocessing worker\n",
    "    # 'fork' multiprocessing after CUDA init is not work (we must use 'spawn' instead)\n",
    "    # since our pipeline can be re-build (when a dicom can't be decoded on GPU),\n",
    "    # 2 options:\n",
    "    #       (py_num_workers = 0, py_start_method=?)\n",
    "    #       (py_num_workers > 0, py_start_method = 'spawn')\n",
    "    assert not (py_num_workers > 0 and py_start_method == 'fork')\n",
    "\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return decode_crop_save_dali(roi_yolox_engine_path,\n",
    "                                     dcm_paths,\n",
    "                                     save_paths,\n",
    "                                     save_backend=save_backend,\n",
    "                                     batch_size=batch_size,\n",
    "                                     num_threads=num_threads,\n",
    "                                     py_num_workers=py_num_workers,\n",
    "                                     py_start_method=py_start_method,\n",
    "                                     device_id=device_id)\n",
    "    else:\n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_chunk = num_samples // parallel_n_chunks\n",
    "        if num_samples % parallel_n_chunks > 0:\n",
    "            num_samples_per_chunk += 1\n",
    "        starts = [num_samples_per_chunk * i for i in range(parallel_n_chunks)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_chunk, num_samples) for start in starts\n",
    "        ]\n",
    "        if isinstance(device_id, list):\n",
    "            assert len(device_id) == parallel_n_chunks\n",
    "        elif isinstance(device_id, int):\n",
    "            device_id = [device_id] * parallel_n_chunks\n",
    "\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`, {parallel_n_chunks} chunks ...'\n",
    "        )\n",
    "        if parallel_backend == 'joblib':\n",
    "            _ = Parallel(n_jobs=parallel_n_jobs, backend=joblib_backend)(\n",
    "                delayed(decode_crop_save_dali)(\n",
    "                    roi_yolox_engine_path,\n",
    "                    dcm_paths[start:end],\n",
    "                    save_paths[start:end],\n",
    "                    save_backend=save_backend,\n",
    "                    batch_size=batch_size,\n",
    "                    num_threads=num_threads,\n",
    "                    py_num_workers=py_num_workers,  # ram_v3\n",
    "                    py_start_method=py_start_method,\n",
    "                    device_id=worker_device_id,\n",
    "                ) for start, end, worker_device_id in zip(\n",
    "                    starts, ends, device_id))\n",
    "        else:  # manually start multiprocessing's processes\n",
    "            workers = []\n",
    "            daemon = False if py_num_workers > 0 else True\n",
    "            for i in range(parallel_n_jobs):\n",
    "                start = starts[i]\n",
    "                end = ends[i]\n",
    "                worker_device_id = device_id[i]\n",
    "                worker = mp.Process(group=None,\n",
    "                                    target=decode_crop_save_dali,\n",
    "                                    args=(\n",
    "                                        roi_yolox_engine_path,\n",
    "                                        dcm_paths[start:end],\n",
    "                                        save_paths[start:end],\n",
    "                                    ),\n",
    "                                    kwargs={\n",
    "                                        'save_backend': save_backend,\n",
    "                                        'batch_size': batch_size,\n",
    "                                        'num_threads': num_threads,\n",
    "                                        'py_num_workers': py_num_workers,\n",
    "                                        'py_start_method': py_start_method,\n",
    "                                        'device_id': worker_device_id,\n",
    "                                    },\n",
    "                                    daemon=daemon)\n",
    "                workers.append(worker)\n",
    "            for worker in workers:\n",
    "                worker.start()\n",
    "            for worker in workers:\n",
    "                worker.join()\n",
    "    return\n",
    "\n",
    "\n",
    "def _single_decode_crop_save_sdl(roi_extractor,\n",
    "                                 dcm_path,\n",
    "                                 save_path,\n",
    "                                 save_backend='cv2',\n",
    "                                 index=0):\n",
    "    dcm = dicomsdl.open(dcm_path)\n",
    "    meta = DicomsdlMetadata(dcm)\n",
    "    info = dcm.getPixelDataInfo()\n",
    "    if info['SamplesPerPixel'] != 1:\n",
    "        raise RuntimeError('SamplesPerPixel != 1')\n",
    "    else:\n",
    "        shape = [info['Rows'], info['Cols']]\n",
    "\n",
    "    ori_dtype = info['dtype']\n",
    "    img = np.empty(shape, dtype=ori_dtype)\n",
    "    dcm.copyFrameData(index, img)\n",
    "    img_torch = torch.from_numpy(img.astype(np.int16)).cuda()\n",
    "\n",
    "    # YOLOX for ROI extraction\n",
    "    img_yolox = min_max_scale(img_torch)\n",
    "    img_yolox = (img_yolox * 255)  # float32\n",
    "    # @TODO: subtract on large array --> should move after F.interpolate()\n",
    "    if meta.invert:\n",
    "        img_yolox = 255 - img_yolox\n",
    "    # YOLOX infer\n",
    "    try:\n",
    "        xyxy, _area_pct, _conf = roi_extractor.detect_single(img_yolox)\n",
    "        if xyxy is not None:\n",
    "            x0, y0, x1, y1 = xyxy\n",
    "            crop = img_torch[y0:y1, x0:x1]\n",
    "        else:\n",
    "            crop = img_torch\n",
    "    except:\n",
    "        print('ROI extract exception!')\n",
    "        crop = img_torch\n",
    "\n",
    "    # apply voi lut\n",
    "    if meta.window_widths:\n",
    "        crop = apply_windowing(crop,\n",
    "                               window_width=meta.window_widths[0],\n",
    "                               window_center=meta.window_centers[0],\n",
    "                               voi_func=meta.voilut_func,\n",
    "                               y_min=0,\n",
    "                               y_max=255,\n",
    "                               backend='torch')\n",
    "    else:\n",
    "        print('No windowing param!')\n",
    "        crop = min_max_scale(crop)\n",
    "        crop = crop * 255\n",
    "\n",
    "    if meta.invert:\n",
    "        crop = 255 - crop\n",
    "    crop = resize_and_pad(crop, MODEL_INPUT_SIZE)\n",
    "    crop = crop.to(torch.uint8)\n",
    "    crop = crop.cpu().numpy()\n",
    "    save_img_to_file(save_path, crop, backend=save_backend)\n",
    "\n",
    "\n",
    "def decode_crop_save_sdl(roi_yolox_engine_path,\n",
    "                         dcm_paths,\n",
    "                         save_paths,\n",
    "                         save_backend='cv2'):\n",
    "    \"\"\"DicomSDL decoding --> ROI cropping --> norm --> save as 8-bits PNG\"\"\"\n",
    "    \n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    roi_detector = roi_extract.RoiExtractor(engine_path=roi_yolox_engine_path,\n",
    "                                            input_size=ROI_YOLOX_INPUT_SIZE,\n",
    "                                            num_classes=1,\n",
    "                                            conf_thres=ROI_YOLOX_CONF_THRES,\n",
    "                                            nms_thres=ROI_YOLOX_NMS_THRES,\n",
    "                                            class_agnostic=False,\n",
    "                                            area_pct_thres=ROI_AREA_PCT_THRES,\n",
    "                                            hw=ROI_YOLOX_HW,\n",
    "                                            strides=ROI_YOLOX_STRIDES,\n",
    "                                            exp=None)\n",
    "    print('ROI extractor (YOLOX) loaded!')\n",
    "    for i in tqdm(range(len(dcm_paths))):\n",
    "        _single_decode_crop_save_sdl(roi_detector, dcm_paths[i], save_paths[i],\n",
    "                                     save_backend)\n",
    "\n",
    "    del roi_detector\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return\n",
    "\n",
    "\n",
    "def decode_crop_save_sdl_parallel(roi_yolox_engine_path,\n",
    "                                  dcm_paths,\n",
    "                                  save_paths,\n",
    "                                  save_backend='cv2',\n",
    "                                  parallel_n_jobs=2,\n",
    "                                  parallel_n_chunks=4,\n",
    "                                  joblib_backend='loky'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return decode_crop_save_sdl(roi_yolox_engine_path, dcm_paths,\n",
    "                                    save_paths, save_backend)\n",
    "    else:\n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_chunk = num_samples // parallel_n_chunks\n",
    "        if num_samples % parallel_n_chunks > 0:\n",
    "            num_samples_per_chunk += 1\n",
    "        starts = [num_samples_per_chunk * i for i in range(parallel_n_chunks)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_chunk, num_samples) for start in starts\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{joblib_backend}`, {parallel_n_chunks} chunks...'\n",
    "        )\n",
    "        _ = Parallel(n_jobs=parallel_n_jobs, backend=joblib_backend)(\n",
    "            delayed(decode_crop_save_sdl)(roi_yolox_engine_path,\n",
    "                                          dcm_paths[start:end],\n",
    "                                          save_paths[start:end], save_backend)\n",
    "            for start, end in zip(starts, ends))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
